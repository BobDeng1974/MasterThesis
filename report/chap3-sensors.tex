%file included in thesis.tex


\chapter{Sensors}
\label{chap3-sensors}

\section{2D Sensors}

\subsection{Laser Range Finder}


\subsubsection{Noise- and Error Sources}



\section{3D Sensors}
There are many different depth sensors available at the consumer market. This section will
outline some of them, and describe the \emph{MESA Imagining} SR-3000 in more detail.
The development of depth-sensing cameras and devices have accelerated the last years
because of the increased computational ability and the fact that electronics become
cheaper and cheaper. \cite{low-cost-depthcameras}

During the next years the video game industry most certainly will
incorporate this new technology into video games to take the games to a new level, such as
Microsofts \emph{Project Natal} for Xbox 360 \cite{project-natal}. This uses infrared
projected light stereo. The camera will sense depth by using stereo cameras to sense a
coded near-infrared light pattern, by using standard CMOS image sensors. According to
Microsoft the accuracy in depth will be about 1 cm, and practical ranges will be about
1 to 3.5 meters. The camera will operate at 640x480 pixel resolution at 60 Hz
\cite{conceivably-tech}.





\subsection{Stereo Camera}


\subsubsection{Camera Calibration}
To work with stereo images the captured images need to be rectified, i.e. the images need
to be corrected for distortion introduced by the camera lenses. This is a part of
calibrating the camera, meaning that the intrinsic parameters of the camera are
determined and the lens distortion parameters described in Chapter \ref{chap2}.

To determine the parameters the \emph{Camera Calibration Toolbox} for Matlab is used
\cite{camera-calib-toolbox}. 20 image pairs are captured of a checker board in various
distances and orientation from the stereo rig. This images for left and right camera are
processed individually to calculate the distortions of the individual camera. 
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{pics/left7}
    \caption{Seventh left image in the calibration sequence}
    \label{chap2:fig-checkerboard}
\end{figure}
The four extreme-most corners of the checkerboard is selected in each image, and an
internal algorithm detects the corners of the squares in each image. This gives an initial
guess on how the distortion parameters are. The reprojection error is then minimized with
regard to the distortion parameters using a nonlinear gradient decent algorithm. For more
information on how this is carried out, see \cite{heikkila}.

The distortion parameters for both cameras are summarized in Table
\ref{chap3:tab-distortion-coeffs}.
\begin{table}
  \centering
    \begin{tabular}{|c|c|c|} 
        \hline
                & Left Camera       & Right Camera \\
        \hline
        $k_1$   & $-0.10292 \pm 0.3794$            & $-0.5450 \pm 0.02456$  \\
        $k_2$   & $-0.22555 \pm 0.49387$            & $-0.38334 \pm 0.13037$  \\
        $h_3$   & $ 0$                          & $0$                  \\
        \hline
        $p_1$   & $-0.00361 \pm 0.00141$    & $-0.00033 \pm 0.00115$ \\
        $p_2$   & $0.00206 \pm 0.00174$     & $ 0.000491 \pm 0.00243$  \\
        \hline
    \end{tabular}
    \caption{The distortion parameters discussed in Section \ref{chap2:sec-distortion}}
    \label{chap3:tab-distortion-coeffs}
\end{table}





Figure \ref{chap3:fig-comp-lensdist} below shows how the camera lenses of the Stereo camera distorts the pictures
towards the edges of the lens. 
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.47\textwidth]{pics/left_comp_dist}
    \includegraphics[width=0.47\textwidth]{pics/right_comp_dist}
    \caption{The left and right camera distortion due to lens nonlinearities}
    \label{chap3:fig-comp-lensdist}
\end{figure}
The Figure clearly shows that the principal axis are offset from the center of the images,
and that it is opposite for the left and right images. This means that the view field of
the two cameras will be quite different, and result in a reduced field-of-view for the
total stereo rig.

The intrinsic parameters for the cameras are summarized in Table
\ref{chap3:tab-intrinsic-stereo}. The values are all in pixel coordinates. The real focal
distances can be calculated by knowing the pixel-to-length ratio.
\begin{table}
  \centering
    \begin{tabular}{|c|c|c|} 
        \hline
                & Left Camera       & Right Camera \\
        \hline
        $f_x$   & $877.11\pm 2.94$  & $880.28\pm 3.08$  \\
        $f_y$   & $876.49\pm 2.88$  & $879.20\pm 2.96$  \\
        \hline
        $c_x$   & $350.00\pm 6.32$  & $338.54\pm 6.21$ \\
        $c_y$   & $214.30\pm 4.65$ & $267.59\pm 4.35$  \\
        \hline
    \end{tabular}
    \caption{Intrinsic parameters of the stereo rig in pixel related units including
    uncertainties}
    \label{chap3:tab-intrinsic-stereo}
\end{table}




This distortion can be removed and are called rectifying the image. This makes common
features in the two images aligned horizontally. This makes it much easier for the matching
algorithm because it only need to match vertically.


\subsubsection{Depth resolution}
The depth resolution which can be expected from the Stereo cam are,
\begin{equation}
    \Delta Z = \frac{Z^2}{f T}\Delta d
\end{equation}
where $\Delta d$ is the smallest increment in disparity, $f$ is the focal distance, and
$T$ is the baseline.


\subsubsection{Noise- and Error Sources}




\subsection{Time-of-Flight Camera}
The time of flight camera gives out either Cartesian coordinates or intensity. This
intensity is the amplitude of the returned laser light. Figure
\ref{chap3:fig-tof-amppicture} shows a typical amplitude plot of the inside of a pipe.
\begin{figure}[htbp]
    \centering
   % \includegraphics[width=0.8\textwidth]{pics/tof-amppicture}
    \caption{Typical amplitude image of the SR-3000 ToF-camera}
    \label{chap3:fig-tof-amppicture}
\end{figure}



\subsubsection{Noise- and Error Sources}
*******LIST THE ERROR SOURCES AND HOW TO SUPPRESS THEM**********




\section{Comparisons Between the Proposed Sensors}
\begin{table}[htbp]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
        \hline
        Sensor & Range & Accuracy & FoV & Ang Res & Weight & Scan Freq & Power Cons &  Cost \\
        \hline
        LMS-100 & 20 m & 12 mm &  $270^{\circ}$ & $0.25-0.5^{\circ}$  & 1.1 kg    & 50 Hz & Not specified  & \$5500 \\
        \hline
        LMS-200 & 80 m & 30 mm &  $180^{\circ}$  & $0.25-1^{\circ}$  & 4.5 kg    & 75 Hz & Not specified &  \$5000 \\
        \hline
        UTM-30LX & 30 m & 30 mm & $270^{\circ}$ & $0.25^{\circ}$  & 210 g     & 40 Hz  &$<8$ W   &  \$5000 \\
        \hline
        URG-04LX & 4 m & 10 mm & $240^{\circ}$ & $0.36^{\circ}$ & 160 g  & 10 Hz & ca 2.5 W &  \$2400 \\
        \hline
        URG-04LX-URG01 & 5.6 m & 30 mm & $240^{\circ}$ & $0.36^{\circ}$ & 160 g & 10 Hz & ca 2.5 W & \$1100 \\
        \hline
        SR3000-ToF Camera & 7.5 m & 30 mm & $47.5$ x $ 39.6 ^\circ$ & 176 x 144 & -  & 25
        fps & 18 W max & \$10000 \\
        \hline
    \end{tabular}
    \caption{Comparison between the proposed sensors}
    \label{tab-chap3-sensors}
\end{table}
***FIX TABLE \ref{tab-chap3-sensors}********


\section{Sensor Configurations}
There are a couple of possible sensor configurations available for this project. ***SHOULD
MAYBE BE IN THE IMPLEMENTATION CHAPTER?****

The Laser Range Finder should alway be used in every sensor configuration. This should be
the primary source of length measurements since this is the most accurate of all the range
devices. 

The stereo camera might be looked upon as a cheap type of the Time-of-Flight camera. So
the two possible configurations are:
\begin{itemize}
    \item Laser Range Finder and Stereo Camera
    \item Laser Range Finder and Time-of-Flight Camera
\end{itemize}
The Stereo Camera will give sparse range data, while the Time-of-Flight camera will give
dense range data. 




